<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>关于一次Kafka重复消费问题排查记录的闲谈</title>
    <link href="/2020/12/15/%E5%85%B3%E4%BA%8E%E4%B8%80%E6%AC%A1Kafka%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95%E7%9A%84%E9%97%B2%E8%B0%88/"/>
    <url>/2020/12/15/%E5%85%B3%E4%BA%8E%E4%B8%80%E6%AC%A1Kafka%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95%E7%9A%84%E9%97%B2%E8%B0%88/</url>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;前段时间上线一个新服务，我们的运营在测试的时候，导入了一批数据，结果目标表里的数据是预期数量的2倍，有大量的重复数据，一开始我认为可能是我没有过滤数据类型导致的，我所消费的数据是通过监听数据库的binlog解析后推送到Kafka的数据，我收到kafka消息经过反序列化得到多条数据库表的变动记录，每一条记录都有一个类型：<code>INSERT</code>，<code>UPDATE</code>，<code>DELETE</code>，其实也就是对应SQL的类型，我一开始并没有判断这个类型，而是收到的所有数据都进行后续的处理，运营也说可能是有<code>UPDATE</code>操作的，于是我加了过滤，只处理<code>INSERT</code>类型数据，改好之后想着肯定没问题了，于是让运营再删掉原有数据重新导入一遍，第二天再看结果。   </p><p>&emsp;&emsp;然而第二天，问题依旧……这下可难到我了，想了一会猜测会不会是前面的环节推送的数据就是重复的呢？但是也不能瞎猜，于是我在处理一条数据之前打印上offset，发版后开始观察日志，不一会儿库里出现了重复数据了，我拿到重复的订单号之后去日志中搜索，结果，两条数据的offset是一样的？</p><p><img src="https://oscimg.oschina.net/oscnet/up-a4d97566c877c3ad14dc1ae79d947b9c48e.png" alt="img"></p><p>&emsp;&emsp;看来这真是我消费的问题了，但是之前类似的项目也是同样的消费方式，从来没出现过重复消费呀，这就让我非常纳闷，于是找了几条重复数据，观察了一下插入时间，发现时间间隔还挺有规律，基本都是五分钟左右重复插入一次，在日志上发现重复消费是两台机器交替进行的，我所消费的Topic是只有一个分区的，所以只会有一台机器消费，从日志上看出来两台机器在交替消费，因而产生了重复消费，但是为什么呢，于是再搜了下<em>Exception</em>关键词，发现了一个<code>CommitFailedException</code>：</p><p><img src="https://oscimg.oschina.net/oscnet/up-4a5809d58553eb75c4d899315e717ef0e9c.png" alt="img"></p><p>&emsp;&emsp;可以看出来，是当前消费端被踢出了消费组，随后offset提交失败，然后换了一台机器重新消费这个offset，导致了重复消费，但是为什么被被剔除出消费组我仍然无法解释，因为日志上再也没其他错误信息了，好在后来又让我发现了点蛛丝马迹，那会刚好使用了<code>tail -f</code>命令，不经意间发现过了好久，接收到的数据offset都是一样的，一条kafka消息解压后有这么多？(后来在本地试了下，果然一条Kafka消息，反序列化后都有3000到9000条不等，这也太多了吧。。)经过一番Google之后注意到了Kafka的两个配置项：<code>max.poll.interval.ms</code>和<code>max.poll.records</code>分别代表拉取消息的间隔时间和拉取的最大条数，我的配置是：</p><pre class="line-numbers language-properties" data-language="properties"><code class="language-properties">max.poll.interval.ms &#x3D; 600000 # 默认5分钟max.poll.records &#x3D; 20         # 这个是我最开始写的20条</code></pre><p>&emsp;&emsp;也就是说，最快要5分钟内处理完20个offset，否则将认为客户端不在消费了，也就产生了上面的异常，被踢出消费组，而后又commit失败。从打印的日志来看，这个时间明显不够，处理一个offset的消息都要很久，更别说最大20个了。知道原因后，我改了参数:</p><pre class="line-numbers language-properties" data-language="properties"><code class="language-properties">max.poll.interval.ms &#x3D; 1200000max.poll.records &#x3D; 1</code></pre><p>&emsp;&emsp;也就是一条十分钟的一个的下限速度，但是后来证明，这个时间依旧远远不够(最后这个时间已经改为了1个小时…)，经过一段时间的观察，一条Kafka消息反序列化最多会有10000条数据，处理时间最长大约40多分钟，我最开始确实没有想到这个Topic中的一条消息包含的数据会这么多，导致了这一系列的问题，时间改为1小时后连续两天再没出现过重复消费offset的问题。</p><p>&emsp;&emsp;但是但是，这个程序的处理速度慢，也是导致此问题的一个原因，当然后来发现代码中，处理一条数据，平均查询与插入数据库的次数有五六十次！这当然快不起来啊，于是我在大部分的SQL查询部分使用<code>WeakHashMap</code>进行了数据缓存，因为这部分查询的数据基本是不会有变动的，极大的减少了数据库查询次数，处理速度提升了将近10倍！再后来，由于这个Topic只有一个partition，完全没办法用到多台机器的性能，而且据运维反馈，这个TiDB binlog监听的中间件只支持一个分区发送， 于是我自己在程序中增加了一道转发，即消费到Kafka消息反序列化之后，将反序列化之后的数据先不做处理，直接一条一条转发至RocketMQ，这个过程是非常快的，rocketMQ再发送至各个机器上(也可新建一个多partition的Kafka Topic用于转发)，这样就能充分利用集群的优势，进一步极大地提高处理速度，这一块说这么多其实偏题了，属于后续的一个优化。</p><p>&emsp;&emsp;后来发现，其实每一次在消费端即将离开之前，都会有一条日志：</p><p><img src="https://oscimg.oschina.net/oscnet/up-9bd6fbd5b2048752b177b7de8a7211c6c6d.png" alt="img"></p><p>&emsp;&emsp;提示向服务端发送了离开消费组的请求，因为客户端poll操作已超时，并建议增大最大拉取间隔时间或者减小最大拉取数量(这个不行，我都改到1了 T_T )。  </p><p>&emsp;&emsp;然后在处理完一个offset提交的时候会提示请求失败，当前消费者不再是此消费组的一部分。</p><p><img src="https://oscimg.oschina.net/oscnet/up-ded106931e5c1de8ecefc6f11fca2dbf796.png" alt="img"></p><p>&emsp;&emsp;但是日志级别是INFO，确实不太容易发现，至此问题已完全解决，原因也完全清楚了。</p><p>&emsp;&emsp;总的来说此次遇到的Kafka重复消费的原因，第一是Kafka的消息太大（后来解析到最大有包含25万条数据的一条消息。。。这都是使用protobuf序列化的消息），但是这部分我们无法变动，第二是一开始处理速度也比较慢，默认间隔时间完全不够，综合导致频繁重新消费。</p><p>&emsp;&emsp;经过此次问题对Kafka参数有了更深的一些认识，除上面两个用到的同时也是比较重要参数之外，还有请求的超时时间、会话超时时间、心跳检测事件、拉取消息的超时时间等，在本地Debug期间还遇到过一个总是拉取不到消息然后报空指针异常的问题，然后查到原因是拉取Kafka消息超时了，一条消息可能有十几兆，那会刚好我电脑的网速非常慢，就超时了，后来加大了<code>fetch.max.wait.ms</code>和<code>request.timeout.ms</code>就没问题了。很有意义的一次Kafka问题排查经历</p><h3 id="附一个：Kafka-Consumer配置官方文档"><a href="#附一个：Kafka-Consumer配置官方文档" class="headerlink" title="附一个：Kafka Consumer配置官方文档"></a>附一个：<a href="http://kafka.apache.org/090/documentation.html#consumerconfigs">Kafka Consumer配置官方文档</a></h3>]]></content>
    
    
    
    <tags>
      
      <tag>kafka</tag>
      
      <tag>后端开发</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动态创建与修改定时任务</title>
    <link href="/2020/12/15/%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BA%E4%B8%8E%E4%BF%AE%E6%94%B9%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"/>
    <url>/2020/12/15/%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BA%E4%B8%8E%E4%BF%AE%E6%94%B9%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/</url>
    
    <content type="html"><![CDATA[<h3 id="emsp-emsp-最近遇到一个需求，需要能够按照特定的配置执行定时任务，而且定时任务需要在应用不重启的情况下动态增删改，Spring提供的-Scheduled注解是硬编码形式，只能实现固定的定时任务，随后经过一番探究，依托注明的quartz框架终于实现了该功能，下面来分享一下我的方案。"><a href="#emsp-emsp-最近遇到一个需求，需要能够按照特定的配置执行定时任务，而且定时任务需要在应用不重启的情况下动态增删改，Spring提供的-Scheduled注解是硬编码形式，只能实现固定的定时任务，随后经过一番探究，依托注明的quartz框架终于实现了该功能，下面来分享一下我的方案。" class="headerlink" title="&emsp;&emsp;最近遇到一个需求，需要能够按照特定的配置执行定时任务，而且定时任务需要在应用不重启的情况下动态增删改，Spring提供的@Scheduled注解是硬编码形式，只能实现固定的定时任务，随后经过一番探究，依托注明的quartz框架终于实现了该功能，下面来分享一下我的方案。"></a>&emsp;&emsp;最近遇到一个需求，需要能够按照特定的配置执行定时任务，而且定时任务需要在应用不重启的情况下动态增删改，Spring提供的<code>@Scheduled</code>注解是硬编码形式，只能实现固定的定时任务，随后经过一番探究，依托注明的quartz框架终于实现了该功能，下面来分享一下我的方案。</h3><ul><li><p>首先引入quartz maven依赖：</p><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml">&lt;dependency&gt;    &lt;groupId&gt;org.quartz-scheduler&lt;&#x2F;groupId&gt;    &lt;artifactId&gt;quartz&lt;&#x2F;artifactId&gt;    &lt;!-- 由于我的项目继承了spring-boot-starter-parent，因此这里可以不用写版本号，会直接使用父pom文档中dependencyManagement的quartz版本号 --&gt;    &lt;!-- &lt;version&gt;2.3.2&lt;&#x2F;version&gt; --&gt;&lt;&#x2F;dependency&gt;</code></pre></li><li><p>我的定时任务配置是存储在MySQL数据库当中的，当程序启动时，初始化过程会的<code>init()</code>方法会读取一遍所有有效的定时任务配置，然后将其实例化为一个个对象，一个对象便代表了一个定时任务，我定义的类为 <code>public class ScheduledClauseTriggerEngine implements ClauseTriggerEngine&lt;Void&gt;, Job, AutoCloseable</code>，其中<code>ClauseTriggerEngine</code>为我自定义的接口，因为在项目中除定时触发外还有其他的任务触发方式，不再过多赘述，<code>Job</code>(<code>org.quartz.Job</code>)是quartz框架的一个接口，代表一个任务，在任务调度时，<code>Job</code>的<code>execute(JobExecutionContext context)</code>方法会被调用，用以执行任务内容。所有实例化后的<code>ScheduledClauseTriggerEngine</code>对象会被存在一个<code>Map&lt;Integer, ScheduledClauseTriggerEngine&gt;</code>容器中去，Key为定时任务的id，后续在定时任务增删改的时候，也会同步修改这个Map的内容。</p></li><li><p>创建<code>SchedulerFactoryBean</code>(<code>org.springframework.scheduling.quartz.SchedulerFactoryBean</code>)，并设置JobFactory，由于我这里只使用到SchedulerFactoryBean一次，因此这一小段代码写在构造方法中的，若是全局使用，需要在Spring的Configuration类中专门定义一个SchedulerFactoryBean类型的Bean(比较规范的用法)。</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">SchedulerFactoryBean schedulerFactoryBean &#x3D; new SchedulerFactoryBean();        schedulerFactoryBean.setJobFactory((bundle, scheduler) -&gt;                triggers.get(Integer.parseInt(bundle.getJobDetail().getKey().getName())));        schedulerFactoryBean.afterPropertiesSet();        this.scheduler &#x3D; schedulerFactoryBean.getScheduler();        this.scheduler.start();</code></pre><p><code>setJobFactory</code>这一步比较重要，默认的调度方案是通过反射实现的，即传入一个Job类型的class，然后反射实例化一个此类的对象，再去调用execute方法，通过<code>JobExecutionContext</code>传参，而我的方案是所有任务类都已实例化完成，我希望在触发时直接返回对应的对象实例去执行即可，因此需要去修改JobFactory，<code>triggers</code>是上一步中所说的存储定时任务的Map，而<code>bundle.getJobDetail().getKey().getName()</code>其实就是获取到了任务的key，在我这里其实也就是定时任务id，这个与后面步骤的代码相对应，也就是此时定时任务触发时，会拿到我提前准备好的Map中的对应任务实例去执行，覆盖默认行为。</p></li><li><p>实例化ScheduledClauseTriggerEngine并创建定时任务，下面为比较重要的几行代码：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">&#x2F;&#x2F; 此部分代码为实例化并创建一个定时任务的代码，我将其封装为了一个方法，方便调用，在初始化方法中查询所有定时任务循环调用即可&#x2F;&#x2F; ... 忽略数据库查询及参数校验的过程&#x2F;&#x2F; 创建JobDetailJobDetail jobDetail &#x3D; new JobDetailImpl()              .getJobBuilder()              &#x2F;&#x2F; 任务的id以及group，id为数据库中的id，在上一步设置的JobFactory，当改任务被调度时，会根据此id去获取到要执行的任务              .withIdentity(clauseTriggerId.toString(), SCHEDULE_JOB_GROUP_NAME)              &#x2F;&#x2F; 任务描述，可选              .withDescription(clauseTrigger.getName())              &#x2F;&#x2F; 这里直接传入ScheduledClauseTriggerEngine.class即可              .ofType(ScheduledClauseTriggerEngine.class)              .build();&#x2F;&#x2F; 添加触发器并开始调度任务scheduler.scheduleJob(jobDetail, TriggerBuilder.newTrigger()        &#x2F;&#x2F; 要调度的任务的key        .withIdentity(clauseTriggerId.toString())        &#x2F;&#x2F; 触发周期，triggerConfigToCronExpression是将数据库中的时间配置转换为corn表达式的方法        .withSchedule(CronScheduleBuilder.cronSchedule(triggerConfigToCronExpression(clauseTrigger.getTriggerConfig())))        .build());&#x2F;&#x2F; 此时其实定时任务已经开始生效&#x2F;&#x2F; 后续操作...triggerEngine.setTemplateList(scheduledTemplates);log.info(&quot;add one scheduled clause trigger: &#123;&#125;&quot;, clauseTrigger.getName());return triggerEngine;</code></pre></li><li><p>定时任务的增删改：</p><ul><li><p>根据id新增一个任务：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">public void addTrigger(int triggerId) &#123;  &#x2F;&#x2F; 根据triggerId实例化和启动一个定时任务，并添加到定时任务的Map中去，instanceOneScheduledClauseTriggerEngine就是上一步所封装的方法  ScheduledClauseTriggerEngine triggerEngine &#x3D; triggers.computeIfAbsent(triggerId, k -&gt; instanceOneScheduledClauseTriggerEngine(triggerId));&#125;</code></pre></li><li><p>根据id删除一个任务：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">public void removeTrigger(int clauseTriggerId) &#123;      try &#123;          &#x2F;&#x2F; 生成要删除的jobKey，注意此处必须传入刚才所使用的的group name，否则会使用默认的组名，便无法查询到我们想要删除的任务          JobKey jobKey &#x3D; JobKey.jobKey(String.valueOf(clauseTriggerId), SCHEDULE_JOB_GROUP_NAME);          &#x2F;&#x2F; 移除定时任务          scheduler.deleteJob(jobKey);          &#x2F;&#x2F; 从Map中移除对应的对象          triggers.remove(clauseTriggerId);          log.info(&quot;remove schedule trigger: &#123;&#125;&quot;, jobKey);      &#125; catch (SchedulerException e) &#123;          log.error(e.getMessage(), e);      &#125;&#125;</code></pre></li><li><p>根据id更新一个任务：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">public void updateTrigger(int triggerId) &#123;    &#x2F;&#x2F; 由于我项目中的定时任务可能任务执行内容会变，因此我是将定时任务删除再重新添加，若定时任务只会有触发时间的变化，也可使用rescheduleJob方法只更新触发时间    removeTrigger(triggerId);    addTrigger(triggerId);&#125;</code></pre></li></ul></li><li><p>集群环境下需要注意的地方</p><ul><li><p>多台机器时，我需要只有一台机器执行任务，而其他机器不执行，在此处我使用了Redis作为锁，追求简便，当然也可使用zookeeper。</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">@Overridepublic void execute(JobExecutionContext context) &#123;    String jobKey &#x3D; context.getJobDetail().getKey().toString();    String redisKey &#x3D; JOB_REDIS_PREFIX + jobKey;    &#x2F;&#x2F; 判断是否获取到锁    &#x2F;&#x2F;noinspection ConstantConditions    if(redisTemplate.opsForValue().increment(redisKey) !&#x3D; 1L)&#123;        log.info(&quot;job &#123;&#125; has in running&quot;, jobKey);        return;    &#125;    &#x2F;&#x2F; 注意上面的判断必须放在try-finally块之外，否则会导致一个隐秘的BUG(无论当前机器是否获取到锁，都会执行finally中的方法，释放掉锁，产生错误)    &#x2F;&#x2F; 为锁加上默认过期时间    redisTemplate.expire(redisKey, 3600, TimeUnit.SECONDS);    try &#123;        MDC.put(&quot;traceId&quot;, randomId());        log.info(&quot;execute schedule job: &#123;&#125;&quot;, jobKey);        long l &#x3D; System.currentTimeMillis();        trigger();        log.info(&quot;finish job: &#123;&#125;, used time: &#123;&#125;ms&quot;, jobKey, System.currentTimeMillis()-l);    &#125; catch (Exception ex)&#123;        log.error(ex.getMessage(), ex);    &#125;finally &#123;        MDC.clear();        &#x2F;&#x2F; 释放锁        redisTemplate.delete(redisKey);    &#125;&#125;</code></pre><p>其实在之前，我写了一个注解：<code>@RedisLock</code>，可以通过注解方式直接为某个方法加分布式锁，但是注解不能传入变量，只能传入常量，在这个项目，锁的key是动态的，无法直接使用，便先采用直接写代码的形式，后期可以添加上此功能，通过注解传入SpringEL表达式解析方法入参，就可以实现动态key值了。</p></li><li><p>多台机器定时任务更新问题，当定时任务配置更改时，我需要响应的修改定时任务，但是多台机器，我不能一台一台机器的手动去调用对应的方法，因此我想到了使用redis的发布订阅去完成，因为Redis的默认消息模式是群发模式，刚好符合我的需求，若项目中有MQ，也可配置一个群发的MQ Topic去实现，略微复杂一些。<br>附我所使用的代码，供参考，基于spring-redis：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">@Bean  RedisMessageListenerContainer redisMessageListenerContainer(RedisConnectionFactory connectionFactory,                                                              ScheduledTriggerService scheduledTriggerService,                                                              DwdDataTriggerService dwdDataTriggerService,                                                              ClauseExecuteService clauseExecuteService) &#123;      RedisMessageListenerContainer container &#x3D; new RedisMessageListenerContainer();      container.setConnectionFactory(connectionFactory);      &#x2F;&#x2F;trigger发布订阅      container.addMessageListener((message, bytes) -&gt; &#123;          String body &#x3D; new String(message.getBody(), StandardCharsets.UTF_8);          if (body.startsWith(&quot;scheduled-trigger-change&quot;)) &#123;            Assert.isTrue(body.matches(&quot;^scheduled-trigger-change:((add)|(remove)|(update)):\\d+$&quot;),&quot;invalid scheduled-trigger-change message: &quot; + body);              String[] split &#x3D; body.split(&quot;:&quot;);              String type &#x3D; split[1];              int triggerId &#x3D; Integer.parseInt(split[2]);              switch (type) &#123;                  case &quot;add&quot;:                      scheduledTriggerService.addTrigger(triggerId);                      break;                  case &quot;remove&quot;:                      scheduledTriggerService.removeTrigger(triggerId);                      break;                  case &quot;update&quot;:                      scheduledTriggerService.updateTrigger(triggerId);                      break;                  default:                      scheduledTriggerService.refreshAllTriggerEngine();                      break;              &#125;          &#125; else &#123;              log.info(&quot;receive redis message, topic: &#123;&#125;, body, &#123;&#125;&quot;, new String(message.getChannel()), body);              dwdDataTriggerService.refreshClauseTrigger();          &#125;      &#125;, new ChannelTopic(&quot;trigger-config-change&quot;));      return container;  &#125;</code></pre><p>发送消息：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">redisTemplate.convertAndSend(&quot;trigger-config-change&quot;, &quot;scheduled-trigger-change:update:0&quot;);</code></pre><p>当定时配置变更时，发送redis消息即可。</p><h3 id="总结-本篇文章基于我的实际项目，讲解了借助于quartz框架的定时任务动态增删改的方案，但是因项目而异，我也做了许多定制化的操作，我的思路就是一项定时任务配置对应一个对象实例，任务触发时直接拿到对应的对象实例进行调用，但是quartz框架的默认调度方案不是这样的，所以做了一下调整，此外还增加了集群环境的支持。本篇文章提供一种方案或者说思路，实际使用时还需要大家结合自己的需求进行合理更改或优化，例如当定时任务比较轻量的时候，我认为可不借助于框架，使用轮询也未尝不是一种简单有效的方案。"><a href="#总结-本篇文章基于我的实际项目，讲解了借助于quartz框架的定时任务动态增删改的方案，但是因项目而异，我也做了许多定制化的操作，我的思路就是一项定时任务配置对应一个对象实例，任务触发时直接拿到对应的对象实例进行调用，但是quartz框架的默认调度方案不是这样的，所以做了一下调整，此外还增加了集群环境的支持。本篇文章提供一种方案或者说思路，实际使用时还需要大家结合自己的需求进行合理更改或优化，例如当定时任务比较轻量的时候，我认为可不借助于框架，使用轮询也未尝不是一种简单有效的方案。" class="headerlink" title="总结: 本篇文章基于我的实际项目，讲解了借助于quartz框架的定时任务动态增删改的方案，但是因项目而异，我也做了许多定制化的操作，我的思路就是一项定时任务配置对应一个对象实例，任务触发时直接拿到对应的对象实例进行调用，但是quartz框架的默认调度方案不是这样的，所以做了一下调整，此外还增加了集群环境的支持。本篇文章提供一种方案或者说思路，实际使用时还需要大家结合自己的需求进行合理更改或优化，例如当定时任务比较轻量的时候，我认为可不借助于框架，使用轮询也未尝不是一种简单有效的方案。"></a><strong>总结:</strong> 本篇文章基于我的实际项目，讲解了借助于quartz框架的定时任务动态增删改的方案，但是因项目而异，我也做了许多定制化的操作，我的思路就是一项定时任务配置对应一个对象实例，任务触发时直接拿到对应的对象实例进行调用，但是quartz框架的默认调度方案不是这样的，所以做了一下调整，此外还增加了集群环境的支持。本篇文章提供一种方案或者说思路，实际使用时还需要大家结合自己的需求进行合理更改或优化，例如当定时任务比较轻量的时候，我认为可不借助于框架，使用轮询也未尝不是一种简单有效的方案。</h3></li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>后端开发</tag>
      
      <tag>Java</tag>
      
      <tag>quartz</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Angular使用Http拦截器</title>
    <link href="/2020/12/15/Angular%E4%BD%BF%E7%94%A8Http%E6%8B%A6%E6%88%AA%E5%99%A8/"/>
    <url>/2020/12/15/Angular%E4%BD%BF%E7%94%A8Http%E6%8B%A6%E6%88%AA%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;在前端开发的过程中，一般通过HTTP接口与后端进行数据交互，而后端一般会固定一个返回格式，例如：</p><pre class="line-numbers language-json" data-language="json"><code class="language-json">&#123;    &quot;code&quot;: 0,    &quot;success&quot;: true,    &quot;message&quot;: &quot;&quot;,    &quot;result&quot;:&#123;        &quot;id&quot;:9,        &quot;title&quot;:&quot;&quot;,        &quot;content&quot;:&quot;&quot;,        &quot;createTime&quot;:&quot;2020-11-25 19:22:31&quot;,        &quot;updateTime&quot;:&quot;2020-11-25 19:47:22&quot;,        &quot;available&quot;:true    &#125;&#125;</code></pre><p>&emsp;&emsp;前端在拿到数据后，首先判断是否成功，然后取出数据体显示到页面上，但是每一次都这么去做，几乎都是重复的操作，在后端开发的过程中，有诸如过滤器、Spring提供的拦截器等工具实现对HTTP请求的统一处理，在前端其实也有类似的东西，Angular框架就提供了拦截器接口:</p><pre class="line-numbers language-typescript" data-language="typescript"><code class="language-typescript">interface HttpInterceptor &#123;  intercept(req: HttpRequest&lt;any&gt;, next: HttpHandler): Observable&lt;HttpEvent&lt;any&gt;&gt;&#125;</code></pre><p>&emsp;&emsp;可以实现统一的HTTP请求处理，帮助我们简化代码，并且方便代码的维护，直接放上一个我实际使用过的示例代码：</p><pre class="line-numbers language-typescript" data-language="typescript"><code class="language-typescript">import &#123;  HttpErrorResponse,  HttpEvent,  HttpHandler,  HttpInterceptor,  HttpRequest,  HttpResponse,  HttpResponseBase&#125; from &#39;@angular&#x2F;common&#x2F;http&#39;;import &#123;Observable, of, throwError&#125; from &#39;rxjs&#39;;import &#123;Injectable&#125; from &#39;@angular&#x2F;core&#39;;import &#123;catchError, debounceTime, finalize, mergeMap, retry&#125; from &#39;rxjs&#x2F;operators&#39;;import &#123;AppService&#125; from &#39;..&#x2F;service&#x2F;app.service&#39;;&#x2F;** * 全局HTTP请求拦截器 *&#x2F;@Injectable()export class AppHttpInterceptor implements HttpInterceptor &#123;  &#x2F;&#x2F; 当前正在通信中的HTTP请求数量，此值是为了在http请求的过程中添加加载动画  public processingHttpCount &#x3D; 0;  &#x2F;&#x2F; 依赖注入  constructor(public appService: AppService) &#123;  &#125;  intercept(req: HttpRequest&lt;any&gt;, next: HttpHandler): Observable&lt;HttpEvent&lt;any&gt;&gt; &#123;    &#x2F;&#x2F; &#x2F;monitor前缀的不做处理    if (req.url.includes(&#39;&#x2F;monitor&#39;)) &#123;      return next.handle(req);    &#125;    &#x2F;&#x2F;; setTimeout(() &#x3D;&gt; this.appService.showLoadingBar &#x3D; true);    &#x2F;&#x2F;     this.processingHttpCount ++;    return next.handle(req.clone(&#123;      &#x2F;&#x2F; 为所有拦截的请求添加一个&#39;&#x2F;starry&#39;前缀      url: &#39;&#x2F;starry&#39; + (req.url.startsWith(&#39;&#x2F;&#39;) ? req.url : &#39;&#x2F;&#39; + req.url)    &#125;))      .pipe(        debounceTime(1000),        &#x2F;&#x2F; 失败时重试2次        retry(2),        mergeMap((event: any) &#x3D;&gt; &#123;          &#x2F;&#x2F; 处理后端HTTP接口返回结果          if (event instanceof HttpResponseBase) &#123;            &#x2F;&#x2F; HTTP返回代码正常            if (event.status &gt;&#x3D; 200 &amp;&amp; event.status &lt; 400) &#123;              &#x2F;&#x2F; 处理HTTP Response              if (event instanceof HttpResponse) &#123;                const body &#x3D; event.body;                &#x2F;&#x2F; 判断后端的成功标志字段是否为true                if (body &amp;&amp; body.success) &#123;                  &#x2F;&#x2F; 取出响应体数据的data部分并继续后续操作(将原有的body替换为了body[&#39;result&#39;])                  return of(new HttpResponse(Object.assign(event, &#123;body: body.result&#125;)));                &#125; else &#123;                  &#x2F;&#x2F; 抛出异常                  throw Error(body.message);                &#125;              &#125;            &#125;          &#125;          &#x2F;&#x2F; 其余事件类型不作拦截处理          return of(event);        &#125;), catchError((err: HttpErrorResponse) &#x3D;&gt; &#123;          &#x2F;&#x2F; 如果发生5xx异常，显示一个错误信息提示          this.appService.showSnackBar(err.message, 4000);          console.error(err.message)          return throwError(err);        &#125;), finalize(() &#x3D;&gt; &#123;          &#x2F;&#x2F; 最终processingHttpCount减一，并且在减至0的时候移除掉加载动画          setTimeout(() &#x3D;&gt; --this.processingHttpCount &#x3D;&#x3D;&#x3D; 0 ?            this.appService.showLoadingBar &#x3D; false : this.appService.showLoadingBar &#x3D; true, 500);        &#125;));  &#125;&#125;</code></pre><p>&emsp;&emsp;并将此拦截器注册在Angular模块的provider中去，在providers中添加：</p><pre class="line-numbers language-typescript" data-language="typescript"><code class="language-typescript">&#123;    provide: HTTP_INTERCEPTORS, useClass: AppHttpInterceptor, multi: true&#125;</code></pre><p>&emsp;&emsp;这样在调用后端接口的时候我们不用每次再处理后端的返回数据，可直接在返回值中拿到数据体部分去做页面展示，其余的事情就交给拦截器去处理，例如：</p><pre class="line-numbers language-typescript" data-language="typescript"><code class="language-typescript">this.httpClient.get(&#39;&#x2F;config&#x2F;template&#x2F;&#39; + template).subscribe(data &#x3D;&gt; this.configTemplates.push(&#123;template, data&#125;));</code></pre><p>&emsp;&emsp;至此，就配置好了一个HTTP拦截器，还可以统一处理请求的URL以及异常处理等等，十分方便。官方文档地址：<a href="https://angular.io/api/common/http/HttpInterceptor">https://angular.io/api/common/http/HttpInterceptor</a> (中文文档将 <strong>.io</strong> 改为 <strong>.cn</strong> 即可)<br>&emsp;&emsp;除此之外，对于页面路由，Angular其实也提供了类似的工具，可以对路由以及页面组件进行拦截控制，有机会再作介绍。</p>]]></content>
    
    
    
    <tags>
      
      <tag>前端开发</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>docker安装zookeeper集群(附bash脚本)</title>
    <link href="/2020/12/14/docker%E5%AE%89%E8%A3%85zookeeper%E9%9B%86%E7%BE%A4-%E9%99%84bash%E8%84%9A%E6%9C%AC/"/>
    <url>/2020/12/14/docker%E5%AE%89%E8%A3%85zookeeper%E9%9B%86%E7%BE%A4-%E9%99%84bash%E8%84%9A%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<ul><li><p>准备一个文件夹并进入此文件夹， 例如</p><p><code>mkdir -p ~/docker-app/zookeeper &amp;&amp; cd ~/docker-app/zookeeper</code></p></li><li><p>准备三个文件夹并写入zookeeper的myid</p><p><code>mkdir -p zoo1/data &amp;&amp; echo 1 &gt; zoo1/data/myid</code><br><code>mkdir -p zoo2/data &amp;&amp; echo 2 &gt; zoo2/data/myid</code><br><code>mkdir -p zoo3/data &amp;&amp; echo 3 &gt; zoo3/data/myid</code>  </p></li><li><p>创建一个docker-compose.yml文件并编辑  </p><p><code>vim docker-compose.yml</code></p><p>文件内容如下:  </p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml">version: &#39;3.7&#39;networks:  zk_cluster:    name: zk_cluster #为集群创建一个网络    driver: bridge#集群中的服务(3个节点)services:  zoo1:    image: zookeeper #所使用的镜像    restart: always #容器异常时是否重启    container_name: zoo1 #容器名称    ports: #与宿主机的端口映射      - 2181:2181      - 8001:8080    # zookeeper的配置    environment:      ZOO_MY_ID: 1      ZOO_SERVERS: server.1&#x3D;0.0.0.0:2888:3888;2181 server.2&#x3D;zoo2:2888:3888;2181 server.3&#x3D;zoo3:2888:3888;2181    #容器目录映射，此处使用了相对路径    volumes:      - .&#x2F;zoo1&#x2F;data:&#x2F;data      - .&#x2F;zoo1&#x2F;datalog:&#x2F;datalog    #所使用的网络    networks:      - zk_cluster  zoo2:    image: zookeeper    restart: always    container_name: zoo2    ports:      - 2182:2181      - 8002:8080    environment:      ZOO_MY_ID: 2      ZOO_SERVERS: server.1&#x3D;zoo1:2888:3888;2181 server.2&#x3D;0.0.0.0:2888:3888;2181 server.3&#x3D;zoo3:2888:3888;2181    volumes:      - .&#x2F;zoo2&#x2F;data:&#x2F;data      - .&#x2F;zoo2&#x2F;datalog:&#x2F;datalog    networks:      - zk_cluster  zoo3:    image: zookeeper    restart: always    container_name: zoo3    ports:      - 2183:2181      - 8003:8080    environment:      ZOO_MY_ID: 3      ZOO_SERVERS: server.1&#x3D;zoo1:2888:3888;2181 server.2&#x3D;zoo2:2888:3888;2181 server.3&#x3D;0.0.0.0:2888:3888;2181    volumes:      - .&#x2F;zoo3&#x2F;data:&#x2F;data      - .&#x2F;zoo3&#x2F;datalog:&#x2F;datalog    networks:      - zk_cluster</code></pre></li><li><p>使用docker-compose命令运行  </p><p><code>docker-compose --project-directory $PWD up -d</code></p></li><li><p>附：一键创建docker zookeeper集群并执行的脚本（该方案仅供参考和学习，生产环境需仔细斟酌，合理修改配置参数）</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">#! &#x2F;bin&#x2F;bashmkdir -p zookeeper &amp;&amp; cd zookeeperDIR&#x3D;$PWDecho &quot;cd $DIR&quot;echo &quot;create docker compose config file&quot;CONF_YML&#x3D;&#39;docker-compose.yml&#39;touch $CONF_YMLecho &quot;version: &#39;3.7&#39;&quot; &gt; $CONF_YMLNET&#x3D;&#39;zk_cluster&#39;echo &#39;networks:&#39; &gt;&gt; $CONF_YMLecho &quot;  $NET:&quot; &gt;&gt; $CONF_YMLecho &quot;    name: $NET&quot; &gt;&gt; $CONF_YMLecho &quot;    driver: bridge&quot; &gt;&gt; $CONF_YML#节点个数，可以修改此值创建任意数量的容器节点let CLUSTER_COUNT&#x3D;3 IMAGE&#x3D;zookeeperecho &quot;services:&quot; &gt;&gt; $CONF_YMLfor i in &#96;seq 1 $CLUSTER_COUNT&#96;do    echo &quot;  zoo$i:&quot; &gt;&gt; $CONF_YML    echo &quot;    image: $IMAGE&quot; &gt;&gt; $CONF_YML    echo &quot;    restart: always&quot; &gt;&gt; $CONF_YML    echo &quot;    container_name: zoo$i&quot; &gt;&gt; $CONF_YML    echo &quot;    ports:&quot; &gt;&gt; $CONF_YML    echo &quot;      - $[2180 + $i]:2181&quot; &gt;&gt; $CONF_YML    echo &quot;      - $[8000 + $i]:8080&quot; &gt;&gt; $CONF_YML    echo &quot;    environment:&quot; &gt;&gt; $CONF_YML    echo &quot;      ZOO_MY_ID: $i&quot; &gt;&gt; $CONF_YML    zk_srv_conf&#x3D;&#39;&#39;    for j in &#96;seq 1 $CLUSTER_COUNT&#96;    do        if test $i !&#x3D; $j;        then            zk_srv_conf+&#x3D;&quot;server.$j&#x3D;zoo$j:2888:3888;2181&quot;        else            zk_srv_conf+&#x3D;&quot;server.$j&#x3D;0.0.0.0:2888:3888;2181&quot;        fi        if test $j !&#x3D; $CLUSTER_COUNT;        then            zk_srv_conf+&#x3D;&#39; &#39;;        fi    done    echo &quot;      ZOO_SERVERS: $zk_srv_conf&quot; &gt;&gt; $CONF_YML    echo &quot;    volumes:&quot; &gt;&gt; $CONF_YML    echo &quot;      - .&#x2F;zoo$i&#x2F;data:&#x2F;data&quot; &gt;&gt; $CONF_YML    echo &quot;      - .&#x2F;zoo$i&#x2F;datalog:&#x2F;datalog&quot; &gt;&gt; $CONF_YML    echo &quot;    networks:&quot; &gt;&gt; $CONF_YML    echo &quot;      - $NET&quot; &gt;&gt; $CONF_YML    mkdir -p zoo$i&#x2F;data    echo $i &gt; zoo$i&#x2F;data&#x2F;myiddone# docker pull zookeeperdocker-compose --project-directory $DIR up -d</code></pre></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>docker</tag>
      
      <tag>后端开发</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
